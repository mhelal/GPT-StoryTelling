{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01e2128",
   "metadata": {},
   "source": [
    "## AI for Kids\n",
    "\n",
    "1. Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "!C:\\Users\\manal\\anaconda3\\python.exe -m pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fe54b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\manal\\anaconda3\\lib\\site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/8a/6a/19e9fe04fca059ccf770861c7d5721ab4c2aebc539889e97c7977528a53b/pip-24.0-py3-none-any.whl.metadata\n",
      "  Using cached pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-24.0\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\manal\\anaconda3\\python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21203bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mhelal/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2024-5-28 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (NVIDIA T500, 2048MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1M/14.1M [00:02<00:00, 6.33MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "[ WARN:0@8.624] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@8.624] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     success, img \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 29\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#results.print()  # or .show(), .save(), .crop(), .pandas(), etc.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m    \u001b[38;5;66;03m# results.show()   \u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpandas()\u001b[38;5;241m.\u001b[39mxyxy:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# bounding box\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:856\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    854\u001b[0m     im, f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exif_transpose(im)), \u001b[38;5;28mgetattr\u001b[39m(im, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m, f) \u001b[38;5;129;01mor\u001b[39;00m f\n\u001b[1;32m    855\u001b[0m files\u001b[38;5;241m.\u001b[39mappend(Path(f)\u001b[38;5;241m.\u001b[39mwith_suffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:  \u001b[38;5;66;03m# image in CHW\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# reverse dataloader .transpose(2, 0, 1)\u001b[39;00m\n\u001b[1;32m    858\u001b[0m im \u001b[38;5;241m=\u001b[39m im[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mcvtColor(im, cv2\u001b[38;5;241m.\u001b[39mCOLOR_GRAY2BGR)  \u001b[38;5;66;03m# enforce 3ch input\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# PyTorch Hub\n",
    "import torch\n",
    "import cv2\n",
    "import math \n",
    "import time\n",
    "\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "              ]\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model.names[0] = 'ABC'\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640) # adjust width\n",
    "cap.set(4,480) # adjust height\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    results = model(img)\n",
    "    #results.print()  # or .show(), .save(), .crop(), .pandas(), etc.\n",
    "   # results.show()   \n",
    "    for r in results.pandas().xyxy:\n",
    "        # bounding box\n",
    "        for i in range (len(r.xmin)):\n",
    "            x1, y1, x2, y2 = r.xmin[i], r.ymin[i], r.xmax[i], r.ymax[i] \n",
    "            #print( x1, y1, x2, y2)\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "\n",
    "            # put box in cam\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "\n",
    "            # confidence\n",
    "            confidence = math.ceil((r.confidence[i]*100))/100\n",
    "            #print(\"Confidence --->\",confidence)\n",
    "\n",
    "            # class name\n",
    "            #cls = int(r.class[i])\n",
    "            #print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "            # object details\n",
    "            org = [x1, y1]\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontScale = 1\n",
    "            color = (255, 0, 0)\n",
    "            thickness = 2\n",
    "           \n",
    "            cv2.putText(img,classNames[r[\"class\"][i]] + str(confidence), org, font, fontScale, color, thickness)\n",
    "            #cv2.putText(img,r.class , org, font, fontScale, color, thickness)\n",
    "        \n",
    "    cv2.imshow(\"Webcam\", img) # This will open an independent window\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'): # quit when 'q' is pressed\n",
    "        cap.release()\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows() \n",
    "cv2.waitKey(1) # normally unnecessary, but it fixes a bug on MacOS where the window doesn't close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72505f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"Manal_OAI\")\n",
    "#openai.api_key = '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' # manalorama@gmail.com\n",
    "openai.api_key = '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'  # mhelal@ieee.org  - paid account\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key='sk-proj-iNpfKQYlvgjGloW6M6I2T3BlbkFJG8wU0zp9CE6r0FUrOwOO')#os.getenv(\"OPENAI_API_KEY\"))\n",
    "openai.api_key = 'sk-proj-iNpfKQYlvgjGloW6M6I2T3BlbkFJG8wU0zp9CE6r0FUrOwOO' # IEEE manalorama: 'sk-proj-RFMvAUrhG5krNCZVej2nT3BlbkFJ5XZfKt6aaLDIkLCNurht'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIInitialises = False\n",
    "def initOpenAI ():\n",
    "    if !OpenAIInitialises:\n",
    "        import os\n",
    "        import openai\n",
    "        openai.api_key = os.getenv(\"Manal_OAI\")\n",
    "        #openai.api_key = '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' # manalorama@gmail.com\n",
    "        openai.api_key = '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'  # mhelal@ieee.org  - paid account\n",
    "\n",
    "        import os\n",
    "        from openai import OpenAI\n",
    "\n",
    "        client = OpenAI(api_key='sk-proj-iNpfKQYlvgjGloW6M6I2T3BlbkFJG8wU0zp9CE6r0FUrOwOO')#os.getenv(\"OPENAI_API_KEY\"))\n",
    "        openai.api_key = 'sk-proj-iNpfKQYlvgjGloW6M6I2T3BlbkFJG8wU0zp9CE6r0FUrOwOO' # IEEE manalorama: 'sk-proj-RFMvAUrhG5krNCZVej2nT3BlbkFJ5XZfKt6aaLDIkLCNurht'\n",
    "        OpenAIInitialises = True\n",
    "    return OpenAIInitialises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLLMResponse (prompt, messages=None, voice=False):\n",
    "    if voice == True:\n",
    "        import win32com.client as wincom\n",
    "        speak = wincom.Dispatch(\"SAPI.SpVoice\")\n",
    "    if messages == None:\n",
    "        messages = [\n",
    "         {\"role\": \"system\", \"content\" : \"You‚Äôre a kind helpful assistant\"}\n",
    "        ]\n",
    "    messages.append({\"content\": prompt})\n",
    "    completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "    response = completion.choices[0].message.content\n",
    "    if voice == True:\n",
    "        speak.Speak(response)\n",
    "        text = \"Did that answer you?\"\n",
    "        speak.Speak(text)\n",
    "    return response\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1edf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text chatting\n",
    "while True:\n",
    "    prompt = input(\"User: \")\n",
    "    response = getLLMResponse (prompt)\n",
    "    print(f'ChatGPT: {response}')\n",
    "    print (\"*******************************************************************************************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c1a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voice chatting on windows use builtin TTS\n",
    "import win32com.client as wincom\n",
    "\n",
    "speak = wincom.Dispatch(\"SAPI.SpVoice\")\n",
    "\n",
    "# on linux: https://pypi.org/project/TTS/\n",
    "# or any in https://www.thepythoncode.com/article/convert-text-to-speech-in-python \n",
    "# such as gtts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a231bbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m completion \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      4\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m   messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell the world about the ChatGPT API.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      7\u001b[0m   ]\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# the messages list has a dictionary with 2 keys: roles and content\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# The content is simply the content of the message, while there are three main roles : ‚Äúsystem‚Äù, ‚Äúuser‚Äù, or ‚Äúassistant‚Äù. \u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# The ‚Äúuser‚Äù is the one who gives the instructions\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# The \"system\" role to set the behavior of the assistant \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# The \"assistant\" role to store prior responses.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m response \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Tell the world about the ChatGPT API.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# the messages list has a dictionary with 2 keys: roles and content\n",
    "# The content is simply the content of the message, while there are three main roles : ‚Äúsystem‚Äù, ‚Äúuser‚Äù, or ‚Äúassistant‚Äù. \n",
    "# The ‚Äúuser‚Äù is the one who gives the instructions\n",
    "# The \"system\" role to set the behavior of the assistant \n",
    "# The \"assistant\" role to store prior responses.\n",
    "response = completion.choices[0].message.content\n",
    "print (response)\n",
    "speak.Speak(response)\n",
    "\n",
    "text = \"Did that answer you?\"\n",
    "speak.Speak(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8386273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: WHat is the most interactive use of DAL-E to school children\n",
      "ChatGPT: One interactive use of DAL-E for school children could be creating a storytelling session where DAL-E generates images based on their story prompts. For example, the children can come up with a story about a magical forest, and DAL-E can generate images of various elements such as talking animals, colorful trees, and hidden treasures based on their descriptions. This can spark the children's imagination and engagement in storytelling. Additionally, DAL-E can be used to help students create their own illustrations for school projects or presentations, providing a fun and creative way to express their ideas visually.\n",
      "*******************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    " {\"role\": \"system\", \"content\" : \"You‚Äôre a kind helpful assistant\"}\n",
    "]\n",
    "\n",
    "while True:\n",
    "    content = input(\"User: \")\n",
    "    messages.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "    completion = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "    messages=messages)\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "    print(f'ChatGPT: {response}')\n",
    "    print (\"*******************************************************************************************************************************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d997c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
